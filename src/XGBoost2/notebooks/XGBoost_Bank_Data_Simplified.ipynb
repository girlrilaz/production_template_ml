{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Bank Client's Financial Product subscription using Scikit Learn and XGBoost for imbalance dataset.\n",
    "\n",
    "   - [Alok N Singh](https://github.com/aloknsingh/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "1. [Introduction](#introduction)\n",
    "        \n",
    "2. [Data Set](#dataset)   \n",
    "\n",
    "3. [Statement of Classification Problem](#pb_statement)\n",
    "\n",
    "4. [Software and Tools(Xgboost and Scikit Learn)](#software_tools)\n",
    "\n",
    "5. [Data Exploration](#data_exploration)\n",
    "\n",
    "6. [ML Pipelines for Data Processing](#ml_pipeline)\n",
    "\n",
    "7. [Model Training](#model_training)\n",
    "\n",
    "   7.1 [What and Why of XGBoost](#xgboost)\n",
    "   \n",
    "   7.2 [Metrics for Model Performance](#metrics_perf)\n",
    "   \n",
    "   7.3 [First Attempt at Model Training](#ml_train_1)\n",
    "   \n",
    "   7.4 [Strategy For Better Classifier for the Imbalance Data](#ml_clf_strategy)\n",
    "   \n",
    "   7.5 [Second Attempt at Model Training using Weighted Samples](#ml_train_2)\n",
    "   \n",
    "   7.6 [Third Attempt at Model Training using Weighted Samples and Feature Selection](#ml_train_3)\n",
    "\n",
    "8. [Generalization and Prediction](#prediction)\n",
    "\n",
    "9. [Summary](#summary)\n",
    "\n",
    "10. [Pointers to Other Advanced Techniques](#other_techniques)\n",
    "\n",
    "10. [References](#references)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction <a name=\"introduction\"></a>\n",
    "\n",
    "\n",
    "We will illustrates the Machine Learning classification using the Gradient Boosted Tree. Gradient Boosted Tree,\n",
    "is usually a better choice compare to the logistic regression and other techniques. We will use the real life data set which is highly imbalance i.e the number of positive sample is much less than the number of negative samples.\n",
    "\n",
    "We will walk the user to the the following conceptual steps\n",
    "\n",
    "* Data Set Description.\n",
    "* Exploratory Analysis to understand the data.\n",
    "* Use various preprocessing to clean and prepare the data.\n",
    "* Use naive XGBoost to run the classification.\n",
    "    * Use cross validation to get the model.\n",
    "    * Plot, precision recall curve and ROC curve.\n",
    "* We will then tune it and use weighted positive samples to improve classification performance.\n",
    "* We will also talk about the following advanced techniques.\n",
    "    * Oversampling of majority class and Undersampling of minority class.\n",
    "    * SMOTE algorithms.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Set <a name=\"dataset\"></a>\n",
    "\n",
    "We will use the dataset from [UCI repository for Bank Marketing Data Set](http://archive.ics.uci.edu/ml/datasets/Bank+Marketing).\n",
    "\n",
    "The source of the dataset is \n",
    "\n",
    "[Moro et al., 2014] S. Moro, P. Cortez and P. Rita. A Data-Driven Approach to Predict the Success of Bank Telemarketing. Decision Support Systems, Elsevier, 62:22-31, June 2014\n",
    "\n",
    "\n",
    "## Data Set Infromation\n",
    "\n",
    "The data is related with direct marketing campaigns of a Portuguese banking institution. The marketing campaigns were based on phone calls. Often, more than one contact to the same client was required, in order to access if the product (bank term deposit) would be ('yes') or not ('no') subscribed. \n",
    "\n",
    "\n",
    "### Feature Information\n",
    "\n",
    "###### A. Bank Client Information\n",
    "| col num| feature name| feature description |\n",
    "|--------|:-----------:|:--------------------|\n",
    "|1|  **age** |(numeric)|\n",
    "|2|  **job** | type of job (categorical: 'admin.','blue-collar','entrepreneur','housemaid','management','retired','self-employed','services','student','technician','unemployed','unknown')|\n",
    "|3| **marital**| marital status (categorical: 'divorced','married','single','unknown'; note: 'divorced' means divorced or widowed)|\n",
    "|4| **education** |(categorical: 'basic.4y','basic.6y','basic.9y','high.school','illiterate','professional.course','university.degree','unknown')|\n",
    "|5| **default**| has credit in default? (categorical: 'no','yes','unknown')|\n",
    "|6| **balance**| how much credit card balance|\n",
    "|7| **housing**| has housing loan? (categorical: 'no','yes','unknown')|\n",
    "|8| **loan**| has personal loan? (categorical: 'no','yes','unknown')|\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###### B. Attributes related with the last contact of the current campaign\n",
    "\n",
    "| col num| feature name| feature description |\n",
    "|--------|:-----------:|:--------------------|\n",
    "|9|**contact**| contact communication type (categorical: 'cellular','telephone')| \n",
    "|10|**day**| last contact day of month (categorical: '1', '2', '3', ..., '30', '31')|\n",
    "|11|**month**| last contact month of year (categorical: 'jan', 'feb', 'mar', ..., 'nov', 'dec')|\n",
    "|12|**duration**|last contact duration, in seconds (numeric). Important note: this attribute highly affects the output target (e.g., if duration=0 then y='no'). Yet, the duration is not known before a call is performed. Also, after the end of the call y is obviously known. Thus, this input should only be included for benchmark purposes and should be discarded if the intention is to have a realistic predictive model.|\n",
    "\n",
    "###### C. other attributes\n",
    "\n",
    "| col num| feature name| feature description |\n",
    "|--------|:-----------:|:--------------------|\n",
    "|13|**campaign**|number of contacts performed during this campaign and for this client (numeric, includes last contact)|\n",
    "|14|**pdays**|number of days that passed by after the client was last contacted from a previous campaign (numeric; 999 means client was not previously contacted)|\n",
    "|15|**previous**|number of contacts performed before this campaign and for this client (numeric)|\n",
    "|16|**poutcome**|outcome of the previous marketing campaign (categorical: 'failure','nonexistent','success') |\n",
    "\n",
    "\n",
    "\n",
    "###### E. Output variable (desired target) \n",
    "| col num| feature name| feature description |\n",
    "|--------|:-----------:|:--------------------|\n",
    "|17|**y**| has the client subscribed a term deposit? (binary: 'yes','no')|\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statement of the Classification Problem  <a name=\"pb_statement\"></a>\n",
    "\n",
    "Now we know the schema of the dataset, lets formalize our model building task.\n",
    "\n",
    "1) We will build a machine learning model to predict if a client is likely to subscribed to the a financial product i.e term deposit.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Challenges in building ML model\n",
    "\n",
    "There are many challenges in the machine learning. For example --\n",
    "\n",
    "   * Non Representative data\n",
    "\n",
    "   * Insufficient data.\n",
    "\n",
    "   * Poor quality data.\n",
    "   \n",
    "   * Imbalance data set.\n",
    "\n",
    "   * Irrelevant features.\n",
    "\n",
    "   * Overfitting the model on data\n",
    "\n",
    "   * Underfitting of the model on data\n",
    "\n",
    "   * Whether model will generalize or not\n",
    "\n",
    "   We will explore many of them as we proceed. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Software and Tools (Xgboost and Scikit Learn) <a name=\"software_tools\"></a>\n",
    "\n",
    "##### Python Package Dependencies\n",
    "\n",
    "We will use the [scikit learn package](http://scikit-learn.org/stable/documentation.html) for this task. Next few cells load the libraries needed for this notebook. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pandas for \n",
    "##   1. reading various files into the dataframe\n",
    "##   2. to performa various data manipulation tasks\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# load numpy\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# for preprocessing\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# for custom transformer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "# for creating pipeline\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "\n",
    "# for cross validation\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "# from sklearn import cross_validation\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import cross_val_predict, cross_val_score\n",
    "\n",
    "# for various metrics and reporting\n",
    "from sklearn import metrics \n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# feature selection\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "# xgboost library\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# plot feature importance\n",
    "from xgboost import plot_importance, plot_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load matplot lib for various plotting\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rc(\"font\", size=14)\n",
    "\n",
    "# we will use the seaborn for visually appealing plots\n",
    "import seaborn as sns\n",
    "sns.set() # set the seaborn stylesheet\n",
    "#sns.set(style=\"white\")\n",
    "#sns.set(style=\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration <a name=\"data_exploration\"></a>\n",
    "\n",
    "\n",
    "First step to build predictive model, is to explore data set so that one can get as much insight as possible so that we can do better feature engineering.\n",
    "\n",
    "##### Data Set loading \n",
    "\n",
    "We will use [pandas](https://pandas.pydata.org/) to read data and create dataframe. \n",
    "   \n",
    "   * Pandas support a lot more functionalities for analysis than reading and writing the dataframe. We will use a few of them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'botocore'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_7808/2539957491.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtypes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mbotocore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mibm_boto3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'botocore'"
     ]
    }
   ],
   "source": [
    "#following code is for IBM Watson Studio loading. note if you are not using Watson studion , \n",
    "# one can skip this.\n",
    "\n",
    "import sys\n",
    "import types\n",
    "import pandas as pd\n",
    "from botocore.client import Config\n",
    "import ibm_boto3\n",
    "\n",
    "def __iter__(self): return 0\n",
    "\n",
    "# @hidden_cell\n",
    "# The following code accesses a file in your IBM Cloud Object Storage. It includes your credentials.\n",
    "# You might want to remove those credentials before you share your notebook.\n",
    "client_9ebc573937b647fdaea67b50c5ffa1e9 = ibm_boto3.client(service_name='s3',\n",
    "    ibm_api_key_id='U2v61-Sy7j8htddu5cFZJe9EEgUeCqILurs3Vj9nn-4B',\n",
    "    ibm_auth_endpoint=\"https://iam.ng.bluemix.net/oidc/token\",\n",
    "    config=Config(signature_version='oauth'),\n",
    "    endpoint_url='https://s3-api.us-geo.objectstorage.service.networklayer.com')\n",
    "\n",
    "#body = client_9ebc573937b647fdaea67b50c5ffa1e9.get_object(Bucket='pythondatasciencenotebooks238af3dbc1384aaebbcd5465d478db62',Key='bank.csv')['Body']\n",
    "body = client_9ebc573937b647fdaea67b50c5ffa1e9.get_object(Bucket='pythondatasciencenotebooks238af3dbc1384aaebbcd5465d478db62',Key='bank.csv')['Body']\n",
    "# add missing __iter__ method, so pandas accepts body as file-like object\n",
    "if not hasattr(body, \"__iter__\"): body.__iter__ = types.MethodType( __iter__, body )\n",
    "\n",
    "#df_data_1 = pd.read_csv(body)\n",
    "#df_data_1.head()\n",
    "\n",
    "data_raw_all = pd.read_csv(body, header=0, sep=\";\")\n",
    "\n",
    "data_raw_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load csv file\n",
    "# if you are using notebook on your laptop. use following to load\n",
    "#data_raw_all = pd.read_csv(\"bank.csv\", header=0, sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# preview \n",
    "print(\"(num_rows, num_cols) =\", data_raw_all.shape)\n",
    "print(\"attributes =\", list(data_raw_all.columns))\n",
    "\n",
    "print (\"\\n\\n'Bank data set preview'\")\n",
    "data_raw_all.head()\n",
    "\n",
    "# schema\n",
    "print(\"'Data set schema'\")\n",
    "data_raw_all.dtypes\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration using Seaborn and Matplotlib\n",
    "\n",
    "Lets start with data exploration. As we explore, we hope to \n",
    "\n",
    "* Look at big picture.\n",
    "* Get insights into the data and problem. \n",
    "* If possible, redefine the problem statement based.\n",
    "\n",
    "\n",
    "Usually, for exploratory analysis, one samples input dataset. However, our dataset is small and hence\n",
    "we will put the sample fraction to be 1.0. For the bigger dataset, one may want to change fraction accordingly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean data\n",
    "## cleaning routines\n",
    "def clean_data(df):\n",
    "    \"\"\"\n",
    "     Clean the data for the exploratory analysis\n",
    "     \n",
    "     arguements:\n",
    "     df -- pandas dataframe.\n",
    "     \n",
    "    \"\"\"\n",
    "    # drop the missing data row\n",
    "    data = df.dropna()\n",
    "    \n",
    "    # first convert the day type to object as day is not of int64 type but a categorical type\n",
    "    data['day'] = df.astype('object')\n",
    "\n",
    "   \n",
    "    return data\n",
    "\n",
    "# \n",
    "data_ex = clean_data(data_raw_all.sample(frac=1.0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore output\n",
    "\n",
    "\n",
    "##### Positive and Negative Class Distribution.\n",
    "\n",
    "We know the output i.e client response 'y' can be 'yes' or 'no'. Lets see it's relative frequencies.\n",
    "Since we are interested in predicting when client is going to purchase a term deposit, out positive sample is 'yes' and\n",
    "negative samples is 'no'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x='y', data=data_ex, palette='husl')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can see that we have class imbalances between positive and negative classes (positive samples are around 10% and negative samples are around 90%). The class imbalance is very common in the real dataset. We will see that this imabalance causes problem in our model performance. However we will also explore ways to mitigate it.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  Check if there is the class skew wrt other numerical columns\n",
    "\n",
    "Lets see for the numeric column, on average if there is a pattern in the two classes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean of the numeric features and how it effects output\n",
    "\n",
    "data_ex.groupby('y').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We note that:\n",
    "\n",
    "   *  age, balance, day, compaign are balanced between 'yes' and 'no' classes and hence probably not the best predictors\n",
    "   *  duration feature as explained in the dataset shouldn't be used for prediction and we will not analyse this feature\n",
    "   *  pdays feature indicates that usually the conversion happens if the client are contacted not frequently\n",
    "   *  previous feature indicates that more the number of times client are contacted more likely he will be a positive sample.\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Explore the inputs\n",
    "\n",
    "We can get many insights into the data set by exploring the inputs i.e features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Distribution and corelation of the numeric columns\n",
    "\n",
    "Lets plot the distribution of the features and their cross co-relation to get the information about reduntant features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the scatter plot.\n",
    "num_cols = ['age', 'balance', 'day', 'duration', 'campaign', 'pdays', 'previous']\n",
    "\n",
    "num_cols_with_y = num_cols + ['y']\n",
    "\n",
    "g = sns.pairplot(data_ex[num_cols_with_y], \n",
    "                 hue = 'y',  # yes is green and \n",
    "                 diag_kind='hist',  # histogram plot for diag\n",
    "                 dropna=True,\n",
    "                 markers=[',', ','], # markers for yes and no                 \n",
    "                 palette=sns.color_palette(['red', 'green']), # # yes is green and no is red\n",
    "                 plot_kws={\n",
    "                     's':3 # size of the point\n",
    "                 },\n",
    "                 \n",
    "                )\n",
    "g = g.map_lower(sns.kdeplot, cmap=\"Blues_d\")\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe the following insights\n",
    "\n",
    "* All the columns of interests (i.e diagonal plot) has the predictive power (as none of them are uniform). \n",
    "\n",
    "* The class imbalance is high (i.e green color 'yes' is far less than red colored 'no' in the diag plot)\n",
    "\n",
    "* Non diagonal lower matrix plot contains contour plot for pairwise features.\n",
    "\n",
    "* Non diagonal upper matrix plot contains the corelation between pairwise corelation. For example cell (1 rst row, 3rd column) shows that there is some corelation between 'age' and 'days' feature. So to get the better feel, we will plot the heat map co-relation matrix also.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the corelation between inputs\n",
    "corr = data_ex[num_cols].corr()\n",
    "\n",
    "# plot heatmap\n",
    "sns.heatmap(corr, \n",
    "            xticklabels=corr.columns.values, yticklabels=corr.columns.values,\n",
    "            cmap=sns.light_palette(\"navy\"),\n",
    "           )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We note that:\n",
    "\n",
    "* pdays and previous features are correlated heavily and we should use one of them\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We did various exploratory analysis to get more insights into the dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Test and Train Set.\n",
    "\n",
    "Now it is time to split aside the train and test dataset. Why is it that we makes this decision at this stage?\n",
    "Since if we don't do it and train our model on the whole dataset and then test it on the part of the dataset, we are testing on the subset of data which was used for training and hence we will never know whether our model generalizes well or not.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "To split the dataset into training and testing, we will use scikit learn's utility [train_test_split](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html). It works also with pandas dataframe and we don't have to do any conversion from pandas to numpy data type.\n",
    "\n",
    "For Model Training, we will sub-split train set and only in the end, we will use test set for generalization test.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 10 percent of dataset is used for test and 90 percent for model training\n",
    "data_raw, data_raw_test = train_test_split(data_raw_all, test_size=0.1, random_state=0)\n",
    "\n",
    "#\n",
    "print(\"Size of the training dataset = \", data_raw.shape)\n",
    "print(\"Size of the testing dataset = \", data_raw_test.shape)\n",
    "print(\"\\nSample of the training dataset \\n\")\n",
    "data_raw.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Pipeline for Data Processing <a name=\"ml_pipeline\"></a>\n",
    "\n",
    "\n",
    "\n",
    "Next, we will preprocess the data so that it is more suitable for input to the scikit learn algorithms.\n",
    "\n",
    "* we will use scikit learn transformer and pipeline to create the ML pipeline. This will allow us to use \n",
    " the pro-processing ml pipeline to apply to both training and test dataset. \n",
    "\n",
    "* We will implement ml pipeline for X (i.e the input feature set) and y for the output. We will use various transformer from scikit learn and as well as use our custom transformer\n",
    "\n",
    "    * clean and split the dataset\n",
    "\n",
    "    * encode the categorical features. \n",
    "    \n",
    "    * combine the various new features to create new X.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Helper functions\n",
    "\n",
    "Model training is the iterative process and we would now build various helper function which will be used later on multiple times. We will explain the details while developing each of helper utilities.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### split dataset into data numeric and categorical attributes\n",
    "\n",
    "Lets create the utility to split the input features to categorical and numerical features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_data_attrs_names(data):\n",
    "    \"\"\"\n",
    "     get the categorical inputs and numerical inputs and output column names.\n",
    "     We use the dtype of the data to decide between numerical and categorical attribut\n",
    "     \n",
    "     arguements:\n",
    "     data -- pandas dataframe.\n",
    "     \n",
    "     return:\n",
    "     a dict with the following keys and values\n",
    "     y: list of of the target attribute\n",
    "     X_cat: list of categorical inputs i.e dtype == object\n",
    "     X_num: list of numerical inputs i.e dtype != object \n",
    "     X: list of the combied {X_num, X_cat} inputs\n",
    "    \"\"\"\n",
    "        \n",
    "    all_attribs = list(data.columns.values)\n",
    "    target_attrib = ['y']\n",
    "\n",
    "    # seperate out output column\n",
    "    y = data[target_attrib]\n",
    "    print(\"\\n\\nsample of 'target i.e output attributes'\")\n",
    "    print(y.head())\n",
    "\n",
    "    data_cat = data.select_dtypes(include=['object']).copy()\n",
    "    data_cat = data_cat.drop(target_attrib[0], axis=1)\n",
    "    cat_attribs = list(data_cat.columns.values)\n",
    "    print(\"\\n\\n'sample of categorical attribute and output attributes'\")\n",
    "    print(data_cat.head())\n",
    "\n",
    "    # sep out continous aka scale columns\n",
    "    data_scale = data.select_dtypes(include=[np.number]).copy()\n",
    "    num_attribs = list(data_scale.columns.values)\n",
    "    print(\"\\n\\n'sample of continous (scale) attributes'\")\n",
    "    print(data_scale.head())\n",
    "\n",
    "    # col_X contains all the predictors\n",
    "    X_attribs = list(all_attribs) # copy all cols names\n",
    "    X_attribs.remove(target_attrib[0])\n",
    "\n",
    "    res = {\n",
    "        'y': target_attrib,\n",
    "        'X_cat': cat_attribs,\n",
    "        'X_num': num_attribs,\n",
    "        'X': X_attribs\n",
    "    }\n",
    "    return res\n",
    "    \n",
    "\n",
    "    \n",
    "attrs_map = get_data_attrs_names(data_raw)   \n",
    "print(\"\\n\\n names of the attributes splited according to type\\n\")\n",
    "print(attrs_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### create transformer to select the subset of data\n",
    "\n",
    "We will extends [BaseEstimator](http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html) and [TransformerMixin](http://scikit-learn.org/stable/modules/generated/sklearn.base.TransformerMixin.html) classes from scikit library to create a transformer which will\n",
    "select a list of the columns corresponding to attribute_names.\n",
    "\n",
    "We will use this transformer later to create the pipeline for inputs as well as output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DataFrameSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, attribute_names):\n",
    "        \"\"\"\n",
    "        custom transformer to select the columns \n",
    "        arguements: \n",
    "        attribute_names -- name of the attributes to select\n",
    "        \"\"\"\n",
    "        self.attribute_names = attribute_names\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"no processing to be done for fit\"\"\"\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        returns the subset of the columns corresponding to the attribute_names\n",
    "        X -- input dataset\n",
    "        return -- selected data columns from X\n",
    "        \"\"\"\n",
    "        return X[self.attribute_names].values\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataFrameCatImputer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        custom categorical imputer\n",
    "        source : https://stackoverflow.com/questions/25239958/impute-categorical-missing-values-in-scikit-learn\n",
    "        Columns of dtype are imputed with the most frequest value in the column\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        for c in X:\n",
    "            print(\"al\", c)\n",
    "            \n",
    "        self.fill = pd.Series([X[c].value_counts().index[0]\n",
    "            if X[c].dtype == np.dtype('O') else X[c].mean() for c in X],\n",
    "            index=X.columns)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return X.fillna(self.fill)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create Transformer to create categorical encoding\n",
    "\n",
    "Since ML algorithms works with numbers, we would like to map string or categorical inputs to integers. However, if we map the categorical features to integer than we might be biasing the data.For example, if we map maritial status ('single', 'married', 'divorced', 'unknown') to (0, 1, 2, 3), then we are giving divorced values to have more weights and it is not we intended, so ideally we would like to map categerical values to one hot encoding. \n",
    "\n",
    "\n",
    "Scikit learn has transformers called [LabelEncoder](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html) and [OneHotEncoder](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html) and we could them together to do encoding of categorical features.\n",
    "\n",
    "However, as of scikit-learn 0.19 version, the LabelEncoder and OneHotEncoder transformer can't be used in the Scikit learn together in a pipeline due to a bug.\n",
    "The bug has been fixed in the dev branch of scikit learn by [PR9151](https://github.com/scikit-learn/scikit-learn/pull/9151) and will be available in future releases. Until then, we will just copy paste the content of class CategoricalEncoder.\n",
    "\n",
    "The following transformer is just copy pasted from the \n",
    "dev branch and we recommend user not to go into the details for the purpose of this notebook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Definition of the CategoricalEncoder class, copied from PR #9151.\n",
    "# Just run this cell, or copy it to your code, do not try to understand it (yet).\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.utils import check_array\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from scipy import sparse\n",
    "\n",
    "class CategoricalEncoder(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Encode categorical features as a numeric array.\n",
    "    The input to this transformer should be a matrix of integers or strings,\n",
    "    denoting the values taken on by categorical (discrete) features.\n",
    "    The features can be encoded using a one-hot aka one-of-K scheme\n",
    "    (``encoding='onehot'``, the default) or converted to ordinal integers\n",
    "    (``encoding='ordinal'``).\n",
    "    This encoding is needed for feeding categorical data to many scikit-learn\n",
    "    estimators, notably linear models and SVMs with the standard kernels.\n",
    "    Read more in the :ref:`User Guide <preprocessing_categorical_features>`.\n",
    "    Parameters\n",
    "    ----------\n",
    "    encoding : str, 'onehot', 'onehot-dense' or 'ordinal'\n",
    "        The type of encoding to use (default is 'onehot'):\n",
    "        - 'onehot': encode the features using a one-hot aka one-of-K scheme\n",
    "          (or also called 'dummy' encoding). This creates a binary column for\n",
    "          each category and returns a sparse matrix.\n",
    "        - 'onehot-dense': the same as 'onehot' but returns a dense array\n",
    "          instead of a sparse matrix.\n",
    "        - 'ordinal': encode the features as ordinal integers. This results in\n",
    "          a single column of integers (0 to n_categories - 1) per feature.\n",
    "    categories : 'auto' or a list of lists/arrays of values.\n",
    "        Categories (unique values) per feature:\n",
    "        - 'auto' : Determine categories automatically from the training data.\n",
    "        - list : ``categories[i]`` holds the categories expected in the ith\n",
    "          column. The passed categories are sorted before encoding the data\n",
    "          (used categories can be found in the ``categories_`` attribute).\n",
    "    dtype : number type, default np.float64\n",
    "        Desired dtype of output.\n",
    "    handle_unknown : 'error' (default) or 'ignore'\n",
    "        Whether to raise an error or ignore if a unknown categorical feature is\n",
    "        present during transform (default is to raise). When this is parameter\n",
    "        is set to 'ignore' and an unknown category is encountered during\n",
    "        transform, the resulting one-hot encoded columns for this feature\n",
    "        will be all zeros.\n",
    "        Ignoring unknown categories is not supported for\n",
    "        ``encoding='ordinal'``.\n",
    "    Attributes\n",
    "    ----------\n",
    "    categories_ : list of arrays\n",
    "        The categories of each feature determined during fitting. When\n",
    "        categories were specified manually, this holds the sorted categories\n",
    "        (in order corresponding with output of `transform`).\n",
    "    Examples\n",
    "    --------\n",
    "    Given a dataset with three features and two samples, we let the encoder\n",
    "    find the maximum value per feature and transform the data to a binary\n",
    "    one-hot encoding.\n",
    "    >>> from sklearn.preprocessing import CategoricalEncoder\n",
    "    >>> enc = CategoricalEncoder(handle_unknown='ignore')\n",
    "    >>> enc.fit([[0, 0, 3], [1, 1, 0], [0, 2, 1], [1, 0, 2]])\n",
    "    ... # doctest: +ELLIPSIS\n",
    "    CategoricalEncoder(categories='auto', dtype=<... 'numpy.float64'>,\n",
    "              encoding='onehot', handle_unknown='ignore')\n",
    "    >>> enc.transform([[0, 1, 1], [1, 0, 4]]).toarray()\n",
    "    array([[ 1.,  0.,  0.,  1.,  0.,  0.,  1.,  0.,  0.],\n",
    "           [ 0.,  1.,  1.,  0.,  0.,  0.,  0.,  0.,  0.]])\n",
    "    See also\n",
    "    --------\n",
    "    sklearn.preprocessing.OneHotEncoder : performs a one-hot encoding of\n",
    "      integer ordinal features. The ``OneHotEncoder assumes`` that input\n",
    "      features take on values in the range ``[0, max(feature)]`` instead of\n",
    "      using the unique values.\n",
    "    sklearn.feature_extraction.DictVectorizer : performs a one-hot encoding of\n",
    "      dictionary items (also handles string-valued features).\n",
    "    sklearn.feature_extraction.FeatureHasher : performs an approximate one-hot\n",
    "      encoding of dictionary items or strings.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, encoding='onehot', categories='auto', dtype=np.float64,\n",
    "                 handle_unknown='error'):\n",
    "        self.encoding = encoding\n",
    "        self.categories = categories\n",
    "        self.dtype = dtype\n",
    "        self.handle_unknown = handle_unknown\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"Fit the CategoricalEncoder to X.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape [n_samples, n_feature]\n",
    "            The data to determine the categories of each feature.\n",
    "        Returns\n",
    "        -------\n",
    "        self\n",
    "        \"\"\"\n",
    "\n",
    "        if self.encoding not in ['onehot', 'onehot-dense', 'ordinal']:\n",
    "            template = (\"encoding should be either 'onehot', 'onehot-dense' \"\n",
    "                        \"or 'ordinal', got %s\")\n",
    "            raise ValueError(template % self.handle_unknown)\n",
    "\n",
    "        if self.handle_unknown not in ['error', 'ignore']:\n",
    "            template = (\"handle_unknown should be either 'error' or \"\n",
    "                        \"'ignore', got %s\")\n",
    "            raise ValueError(template % self.handle_unknown)\n",
    "\n",
    "        if self.encoding == 'ordinal' and self.handle_unknown == 'ignore':\n",
    "            raise ValueError(\"handle_unknown='ignore' is not supported for\"\n",
    "                             \" encoding='ordinal'\")\n",
    "\n",
    "        X = check_array(X, dtype=np.object, accept_sparse='csc', copy=True)\n",
    "        n_samples, n_features = X.shape\n",
    "\n",
    "        self._label_encoders_ = [LabelEncoder() for _ in range(n_features)]\n",
    "\n",
    "        for i in range(n_features):\n",
    "            le = self._label_encoders_[i]\n",
    "            Xi = X[:, i]\n",
    "            if self.categories == 'auto':\n",
    "                le.fit(Xi)\n",
    "            else:\n",
    "                valid_mask = np.in1d(Xi, self.categories[i])\n",
    "                if not np.all(valid_mask):\n",
    "                    if self.handle_unknown == 'error':\n",
    "                        diff = np.unique(Xi[~valid_mask])\n",
    "                        msg = (\"Found unknown categories {0} in column {1}\"\n",
    "                               \" during fit\".format(diff, i))\n",
    "                        raise ValueError(msg)\n",
    "                le.classes_ = np.array(np.sort(self.categories[i]))\n",
    "\n",
    "        self.categories_ = [le.classes_ for le in self._label_encoders_]\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"Transform X using one-hot encoding.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape [n_samples, n_features]\n",
    "            The data to encode.\n",
    "        Returns\n",
    "        -------\n",
    "        X_out : sparse matrix or a 2-d array\n",
    "            Transformed input.\n",
    "        \"\"\"\n",
    "        X = check_array(X, accept_sparse='csc', dtype=np.object, copy=True)\n",
    "        n_samples, n_features = X.shape\n",
    "        X_int = np.zeros_like(X, dtype=np.int)\n",
    "        X_mask = np.ones_like(X, dtype=np.bool)\n",
    "\n",
    "        for i in range(n_features):\n",
    "            valid_mask = np.in1d(X[:, i], self.categories_[i])\n",
    "\n",
    "            if not np.all(valid_mask):\n",
    "                if self.handle_unknown == 'error':\n",
    "                    diff = np.unique(X[~valid_mask, i])\n",
    "                    msg = (\"Found unknown categories {0} in column {1}\"\n",
    "                           \" during transform\".format(diff, i))\n",
    "                    raise ValueError(msg)\n",
    "                else:\n",
    "                    # Set the problematic rows to an acceptable value and\n",
    "                    # continue `The rows are marked `X_mask` and will be\n",
    "                    # removed later.\n",
    "                    X_mask[:, i] = valid_mask\n",
    "                    X[:, i][~valid_mask] = self.categories_[i][0]\n",
    "            X_int[:, i] = self._label_encoders_[i].transform(X[:, i])\n",
    "\n",
    "        if self.encoding == 'ordinal':\n",
    "            return X_int.astype(self.dtype, copy=False)\n",
    "\n",
    "        mask = X_mask.ravel()\n",
    "        n_values = [cats.shape[0] for cats in self.categories_]\n",
    "        n_values = np.array([0] + n_values)\n",
    "        indices = np.cumsum(n_values)\n",
    "\n",
    "        column_indices = (X_int + indices[:-1]).ravel()[mask]\n",
    "        row_indices = np.repeat(np.arange(n_samples, dtype=np.int32),\n",
    "                                n_features)[mask]\n",
    "        data = np.ones(n_samples * n_features)[mask]\n",
    "\n",
    "        out = sparse.csc_matrix((data, (row_indices, column_indices)),\n",
    "                                shape=(n_samples, indices[-1]),\n",
    "                                dtype=self.dtype).tocsr()\n",
    "        if self.encoding == 'onehot-dense':\n",
    "            return out.toarray()\n",
    "        else:\n",
    "            return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create Transformer to create label encoding for output\n",
    "\n",
    "As explained in the previous section, there seems to be a bug in scikit learn current version which doesn't allow\n",
    "Label encoder to be used in a ML pipeline. To solve that we will create our custom transformer, which is same as the \n",
    "original one i.e LabelEncoder but with the signature of fit and transform different. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## custom LabelEncoder as the org doesn't work\n",
    "class MyLabelEncoder(preprocessing.LabelEncoder):\n",
    "    \"\"\"\n",
    "     custom LabelEncoder as the org doesn't work with the pipeline\n",
    "    \"\"\"\n",
    "    def fit_transform(self, X, y=None):\n",
    "        return super(MyLabelEncoder, self).fit_transform(X)\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return super(MyLabelEncoder, self).fit(X)\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return super(MyLabelEncoder, self).transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Utility for ML pipeline for X i.e input features\n",
    "\n",
    "\n",
    "The idea of combining various transformers into a pipeline allows scikit users to create complex data and ML processing\n",
    "pipeline. For the input features we will do the following \n",
    "\n",
    "* Numeric attributes: \n",
    "    * select numerical attributes.\n",
    "    * impute missing values of a feature using the median of the feature.\n",
    "    * scale values of a feature by shifting it by it's mean and dividing by standard deviation.\n",
    "* Categorical attributes:\n",
    "    * select categorical attributes.\n",
    "    * for each of the columns perform one hot encoding\n",
    "* Feature Union:\n",
    "    * combine the columns from numeric and categorical attributes to create the processed input dataset i.e X\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_X_ml_pipeline(cat_attrs, num_attrs):\n",
    "    \"\"\"\n",
    "    Create ml pipeline for the features by first processing numerical and then categorical attribute.\n",
    "    Afterwards, it combines those to create final transformed X attribute.\n",
    "    \n",
    "    arguement:\n",
    "    cat_attrs -- list of categorical attribute\n",
    "    num_attrs -- list of numerical attribute\n",
    "    \n",
    "    return: a full X ML pipeline, which can be used for train and test data\n",
    "    \"\"\"\n",
    "    \n",
    "    num_pipeline = Pipeline([\n",
    "       ('selector', DataFrameSelector(num_attrs)),\n",
    "       ('imputer', preprocessing.Imputer(strategy=\"median\")),\n",
    "       ('std_scaler', preprocessing.StandardScaler()),\n",
    "    ])\n",
    "\n",
    "    cat_pipeline = Pipeline([\n",
    "        ('selector', DataFrameSelector(cat_attrs)),\n",
    "        ('cat_enc', CategoricalEncoder(encoding=\"onehot-dense\")),\n",
    "    ])\n",
    "    \n",
    "    full_pipeline = FeatureUnion(transformer_list=[\n",
    "        ('num_pipeline', num_pipeline),\n",
    "        ('cat_pipeline', cat_pipeline),\n",
    "    ])\n",
    "    \n",
    "    return full_pipeline\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Utility for  ML pipeline for Y i.e output features\n",
    "\n",
    "\n",
    "Output contains 'yes' and 'no' values and should be mapped to numerical values. Lets us custom transformer MyLabelEncoder for this purpose. Also we will be use our custom transformer DataFrameSelector to select output column.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_y_ml_pipeline(target_attr):\n",
    "    \"\"\"\n",
    "    create ml pipeline for the output feature i.e target attribute by first selecting it and mapping 'no' and 'yes' to\n",
    "    0 and 1\n",
    "    arguements:\n",
    "    target_attr -- list of outputs.(Usually a list containing one value)\n",
    "    return:\n",
    "    a full y ML pipeline, which can be used for train and test data\n",
    "    \"\"\"\n",
    "    target_pipeline = Pipeline([\n",
    "        ('selector', DataFrameSelector(target_attr)),\n",
    "        #('cat_imputer', DataFrameCatImputer()),\n",
    "        ('label_enc', MyLabelEncoder()),\n",
    "    ])\n",
    "    \n",
    "    return target_pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create and run the ML pipeline for X and y\n",
    "\n",
    "Now we have defined the function for creating ML pipelines, lets us it to create X and y ML pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "First lets create the X ML pipeline and print it's values.\n",
    "Readers should note that the values are all scaled and one hot encoded.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## run the pipeline\n",
    "\n",
    "X_pipeline = create_X_ml_pipeline(attrs_map['X_cat'], attrs_map['X_num'])\n",
    "X_pipeline = X_pipeline.fit(data_raw)\n",
    "X = X_pipeline.transform(data_raw)\n",
    "\n",
    "\n",
    "print(\"\\n\\nShape of the transformed data. Note that increased in the number of columns due to one hot encoding\\n\")\n",
    "print(\"\\nOriginal data shape (num_rows, num_cols) == \")\n",
    "print(data_raw.shape)\n",
    "print(\"\\nTransformed data shape (num_rows, num_cols) == \")\n",
    "print(X.shape)\n",
    "print(\"\\n\\n Sample of transformed inputs\\n\", X, \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"Output attribute name = \", attrs_map['y'], \"\\n\")\n",
    "\n",
    "y_pipeline = create_y_ml_pipeline(attrs_map['y'])\n",
    "y_pipeline = y_pipeline.fit(data_raw)\n",
    "y = y_pipeline.transform(data_raw)\n",
    "\n",
    "\n",
    "print(\"\\nSample of original output = \", data_raw['y'].as_matrix().reshape(-1)[:20], \"\\n\")\n",
    "print(\"\\nSample of transformed output = \", y[:20], \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training <a name=\"model_training\"></a>\n",
    "\n",
    "we have transformed dataset using our custom ML pipelines and are ready to build the model. There are many ML models in python. We will use [XGBoost](http://xgboost.readthedocs.io/en/latest/). \n",
    "\n",
    "\n",
    "### What and Why of XGBoost. <a name=\"xgboost\"></a>\n",
    "\n",
    "* XGBoost is extreme gradient boosting algorithm based on trees and tends to perform very good out of the box compare to other ML algorithms.\n",
    "* XGBoost is popular amongt data-scientist and one of the most common ML algorithms used in [Kaggle](https://www.kaggle.com/) Competitions.\n",
    "* XGBoost allows one to tune various parameters.\n",
    "* XGBoost allows parallel processing.\n",
    "* Here is the official tutorial, which explains in details [XGBoost Tutorial 1](http://xgboost.readthedocs.io/en/latest/model.html) and also [XGBoost tutorial 2](https://www.slideshare.net/ShangxuanZhang/kaggle-winning-solution-xgboost-algorithm-let-us-learn-from-its-author) is excellent.\n",
    "\n",
    "Though above tutorial is very detail, here we present the basic of XGBoost for the sake of continuation of the text.\n",
    "\n",
    "XGBoost was developed by Tianqi Chen.XGboost is a class of ML algorithms which uses Gradient Boosting.\n",
    "\n",
    "Gradient Boosting in  is an ensemble techniques, at each steps new models are added to correct the errors made by current existing models and thus models are added sequentially until no more improve is possible.\n",
    "\n",
    "Each of the tree model is classification and regression tree (CART). The ensemble of  prediction of multiple trees gives better result compare to the individual tree.\n",
    "\n",
    "\n",
    "The objective of XGBoost model is given by \n",
    "\n",
    "\n",
    "| Objective => | **Obj** **= Loss + Regularization**|\n",
    "|-----|:------------------------|\n",
    "|**Loss** | is the loss function that controls the predictive power. Usually logloss for classification and RMSE loss for regression.|\n",
    "|**Regularization**| controls simplicity and overfitting and depends on the number of leaves|\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create the utils for instantiating classifier with correct options\n",
    "\n",
    "Since we will be instantiating XGBoost classifier multiple times for doing the cross validation and tesitng. Lets create a utility for instantiating the classifier.\n",
    "\n",
    "\n",
    "Here are the explaination in reference to our dataset for each tuning parameters we will use. (Note: also see [XGBoost parameter official guide](https://github.com/dmlc/xgboost/blob/master/doc/parameter.md)\n",
    "\n",
    "| xgboost parameter | description |\n",
    "|:------|:------------|\n",
    "|**booster**| select the type of model to run at each iternation we have the options of tree and linear models|   \n",
    "|**objective**|since we want the output to have the probability also, we will use the logistic objective.|   \n",
    "|**eval_metric**|lets use the error as the eval metrics i.e in each boosting steps we will reduce error.|   \n",
    "|**eta**|eta is like learning rate and it makes the model more robust by shrinking the weights at each iteration.|   \n",
    "|**gamma**|gamma controls the minimum loss reduction to split and it should be tuned.|\n",
    "|**max_depth**|maximum depth of a tree to control the over fitting. should be tuned with cv|\n",
    "|**min_child_weight**|minimum number of samples for the leaf and is used to control overfitting. We will use lower values, as we have class imbalance and if we set high then accuracy of minory class will be affected.|   \n",
    "|**max_delta_step**|maximum delta step from previous iteration for each tree. Higher the value (i.e non zero), more conservative we are!| \n",
    "|**subsample**|as explained before in boosting each tree is build using samples from prev iteration with replace and this specify the fraction of data to be used for each tree. typically values slightly less than 1 makes it robust.|\n",
    "|**colsample_bytree**|as explained before boosting use subset of rows and also subset of columns. this controls the subset of cols as fraction.|\n",
    "|**silent**|control the verbosity.|\n",
    "|**seed**|random seed for reproducibility.| \n",
    "|**base_score**|set the initial prediction score i.e global bias.| \n",
    "|**scale_pos_weight**|how much weight to give the positive sample, in future we will change it but for now lets put it 1|\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_xgb_clf(clf_param=None):\n",
    "    \n",
    "    \"\"\"\n",
    "    create the xgboost classifier with predefined parameters, user can overwright it by passing kw args\n",
    "    \"\"\"\n",
    "   \n",
    "    param = {}\n",
    "    # select the type of model to run at each iternation we have the options of tree and linear models\n",
    "    ##param['booster'] = 'gbtree'\n",
    "    \n",
    "    # since we want the output to have the probability also, we will use the logistic objective.\n",
    "    ##param['objective'] = 'binary:logistic'\n",
    "    \n",
    "    # lets use the error as the eval metrics i.e in each boosting steps we will reduce error\n",
    "    ##param[\"eval_metric\"] = \"error\"\n",
    "    \n",
    "    # eta is like learning rate and it makes the model more robust by shrinking the weights at each iter\n",
    "    ##param['eta'] = 0.3\n",
    "    \n",
    "    # gamma controls the minimum loss reduction to split and it should be tuned.\n",
    "    ##param['gamma'] = 0\n",
    "\n",
    "    # maximum depth of a tree to control the over fitting. should be tuned with cv\n",
    "    ##param['max_depth'] = 10\n",
    "    \n",
    "    # minimum number of samples for the leaf and is used to control overfitting. We will use lower values, as we have\n",
    "    # class imbalance and if we set high then accuracy of minory class will be affected\n",
    "    ##param['min_child_weight']=1\n",
    "    \n",
    "    # maximum delta step from previous iteration for each tree. Higher the value (i.e non zero), more conservative we are\n",
    "    ##param['max_delta_step'] = 0\n",
    "    \n",
    "    # as explained before in boosting each tree is build using samples from prev iteration with replace and this specify \n",
    "    # the fraction of data to be used for each tree. typically values slightly less than 1 makes it robust.\n",
    "    ##param['subsample']= 1\n",
    "    \n",
    "    \n",
    "    # as explained before boosting use subset of rows and also subset of columns. this controls the subset of cols as fraction\n",
    "    ##param['colsample_bytree']=1\n",
    "    \n",
    "    # control the verbosity\n",
    "    ##param['silent'] = 0\n",
    "    \n",
    "    # random seed for reproducibility\n",
    "    ##param['seed'] = 0\n",
    "    \n",
    "    # set the initial prediction score i.e global bias \n",
    "    ##param['base_score'] = 0.5\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    # how much weight to give the positive sample, in future we will change it but for now lets put it 1\n",
    "    param['scale_pos_weight']= 1\n",
    "\n",
    "    if clf_param:\n",
    "       for k, v in clf_param.items():\n",
    "          param[k] = clf_param[k]\n",
    "            \n",
    "    xgb_model = XGBClassifier(**param)\n",
    "    return xgb_model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics for Model Performance <a name=\"metrics_perf\"></a>\n",
    "\n",
    "To come up with the best model, we should evaluate model performance for comparison amongs models.There are many ways to evaluate the model performance for classification.\n",
    "\n",
    "1) Accuracy Score\n",
    "\n",
    "2) Confusion Matrix\n",
    "\n",
    "3) ROC curve\n",
    "\n",
    "4) Precision Recall Curve\n",
    "\n",
    "We will discuss and create utilities for the above in next few sections\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Accuracy Score\n",
    "Accuracy is just the ratio of the number of correctly classified samples to the total number of samples.\n",
    "This is very simple score but it in case of the class imbalance, the score can be misleading. Hence, we \n",
    "will not use this.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  Confusion Matrix\n",
    "\n",
    "Apart from the accuracy, the following performance measures for the models are also very helpful to measure it's performance on test dataset.\n",
    "\n",
    "* The number of samples correctly predicted as negative i.e True Negative (TN).\n",
    "\n",
    "* The number of samples correctly predicted as positive i.e True Positive (TP). These are also used in accuracy calculation.\n",
    "\n",
    "* The number of samples which were positive but were predicted as negative i.e False Negative (FN).\n",
    "\n",
    "* The number of samples which were negative but were predicted as positive i.e False Positive (FP).\n",
    "\n",
    "[Confusion matrix](https://en.wikipedia.org/wiki/Confusion_matrix) gives above mentioned metrics of performance and is widely used for classification task.\n",
    "\n",
    "We have created a utility to visualize confusion matrix based on the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the confusion matrix\n",
    "def plot_cm(y_test_act, y_test_pred, title=\"Confusion Matrix\", cmap=\"Blues\"):\n",
    "    \"\"\"\n",
    "    plot the confusion matrix given the test label and predicted label.\n",
    "    \n",
    "    @arguements:\n",
    "    y_test_act -- actual label (0 or 1) of the data set.\n",
    "    y_test_pred -- model's predicted label (o or 1) of the data set.\n",
    "    title -- title string to be put on plot. default -- Confusion Matrix\n",
    "    cmap -- matplotlib color palette to be used. default -- Blues\n",
    "    \"\"\"\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    cm = confusion_matrix(y_test_act, y_test_pred)\n",
    "    tp = cm[1,1]\n",
    "    tn = cm[0,0]\n",
    "    fp = cm[0,1]\n",
    "    fn = cm[1,0]\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", linewidths=.5, cmap = cmap, ax = ax)\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(\"Predicted class\")\n",
    "    ax.set_ylabel(\"Actual class\")\n",
    "    plt.show()\n",
    "    \n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### ROC curve\n",
    "\n",
    "Lets see what is ROC curve? We can get the TP, TN, FN, FP from confusion matrix as explained in previous section. ML models also gives us the probability of a sample being a positive or negative.  This allows us to make the better decision. \n",
    "\n",
    "For ROC curve, we use \n",
    "\n",
    "**TPR** : True Positive Rate (aka Recall) = $ \\frac{TP}{(TP+FN)} $ i.e number of samples, model correctly classifies a positive sample as positive.\n",
    "\n",
    "**FPR** : False Positive Rate (aka Specificity) = $\\frac{FP}{(FP + TN)}$ i.e number of samples, model incorrectly classifies a negative sample as positve.\n",
    "\n",
    "obviously, we would like to have higher TPR and lower FPR.\n",
    "\n",
    "For example, consider two predicted positive samples, one with 0.55 probability and other with 0.95 probability.\n",
    "These samples are considered positive  because by default the threshold is 0.5. \n",
    "If we use the threshold of 0.6 then first sample will be negative and the second will be positive. \n",
    "\n",
    "In short, the values of TP, FP, TN, FN will change depending on the threshold.\n",
    "\n",
    "Thus we see that by adjusting the threshold, one can change the model's performance and for binary classification this adjustment is very often done. When we adjust the threshold, the value of TPR and FPR will also change, as they are function of TP, FP, TN, FN and when we plot TPR vs FPR for various threhold, we get the ROC curve. \n",
    "\n",
    "For a model to have best performance, we would like to have model rank a randomly chosen positive sample higher than a randomly chosen negative samples and this is what we call Area Under the Curve (AUC) metric.\n",
    "\n",
    "For more information visit wiki for [ROC curve](https://en.wikipedia.org/wiki/Receiver_operating_characteristic).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Precision Recall Curve\n",
    "\n",
    "We have a good measure for measuring model performance using ROC curve. However, when we have the class imbalances, ROC curve doesn't perform good. To understand that see the equation for TPR and FPR and in the case of class imbalances, the number of positive samples are quite few and the number of FN relative to TP is quite more and hence if there is any change in TP or FP, that will be very small compare to TN and FN  and hence even if I get get improvement ROC curve won't be able to capture it.\n",
    "\n",
    "Hence we use another metrics which is **precision** and is defined as **TP/(TP+FP)**. We note that since it is the odd ratio of TP and FP both of them are sensitive to changes in positive samples.This matrix is very useful for measuring the performance in case of class imbalance.\n",
    "\n",
    "But we should note that Precision and recall have inverse relationship and hence if we want high precision then we should be fine with low recall and vice versa. \n",
    "\n",
    "There are many applications where high precision might be desired. For example if we train a model to detect safe website for kids. It's ok to reject many safe website (low recall) as long as we correctly reject bad website (high precision)\n",
    "\n",
    "However for our applications, i.e bank client's CD subscription prediction, higher recall is desirable. Since it is perfectly fine  to have false client i.e model predict client will accept the subscription but will actually decline it, as long as we predict almost all the client who is likely to accept the subscription and thus improving the balance sheet.\n",
    "\n",
    "\n",
    "For more information visit wiki for [Precision and Recall](https://en.wikipedia.org/wiki/Precision_and_recall).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### F1 Score\n",
    "\n",
    "There is tradeoff between precision and recall and hence we need the unified measure to see the effect of precision and recall and F1 score measure both at the same time. It is just the harmonic mean of precision and recall i.e it is defined as $\\frac{1}{\\frac{1}{precision}+ \\frac{1}{recall}}$ or $\\frac{recall \\times precision}{recall + precision}$. \n",
    "\n",
    "For more information visit wiki for [F1 score](https://en.wikipedia.org/wiki/F1_score)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have also created the utility to plot both ROC curve and Precision recall curve given the actual label and predicted probabilities by model on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pr_roc(y_act, y_score, label = \"\", color='b', show=False, tag=\"\"):\n",
    "    \"\"\"\n",
    "    plot both precision recall and ROC curve \n",
    "    arguements:\n",
    "    y_act -- Actual label of the class on the test data.\n",
    "    y_score -- Actual probabilities as predicted by model on the test data.\n",
    "    color (default:blue) -- color of plot\n",
    "    show (default:False) -- flag to control whether to show plot or user will call plt.show() oneself.\n",
    "    \"\"\"\n",
    "    fig = plt.figure(figsize=(20,10))\n",
    "    ax1 = fig.add_subplot(1,2,1)\n",
    "    ax1.set_xlim([-0.025,1.025])\n",
    "    ax1.set_ylim([-0.025,1.025])\n",
    "    ax1.set_xlabel('Recall')\n",
    "    ax1.set_ylabel('Precision')\n",
    "    ax1.set_title('PR Curve ' + tag)\n",
    "\n",
    "    ax2 = fig.add_subplot(1,2,2)\n",
    "    ax2.set_xlim([-0.025,1.025])\n",
    "    ax2.set_ylim([-0.025,1.025])\n",
    "    ax2.set_xlabel('False Positive Rate (FPR)')\n",
    "    ax2.set_ylabel('True Positive Rate (TPR)')\n",
    "    ax2.set_title('ROC Curve ' + tag)\n",
    "\n",
    "    pr,rc,_ = precision_recall_curve(y_act, y_score)\n",
    "    tpr,fpr,_ = roc_curve(y_act, y_score)\n",
    "    \n",
    "    ax1.plot(rc,pr,c=color,label=label)\n",
    "    ax2.plot(tpr,fpr,c=color,label=label)\n",
    "    ax2.plot([0,1], [0,1], 'k--')\n",
    "    \n",
    "    ax1.legend(loc='lower left')    \n",
    "    ax2.legend(loc='lower left')\n",
    "    \n",
    "    if show:\n",
    "        plt.show()\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In previous section, we saw the inverse relationship between precision and recall and how we have to make tradeoff.\n",
    "To get the visual information about the plot, we created a utility to plot precision and recall on the same plot with respect to \n",
    "threshold.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_pr_vs_th(y_act, y_score, show=True, tag=\"\"):\n",
    "    \"\"\"\n",
    "    plot precision and recall vs threshold on same plot\n",
    "    \n",
    "    arguements: \n",
    "    y_act -- Actual label of the class on the test data.\n",
    "    y_score -- Actual probabilities as predicted by model on the test data.\n",
    "    \"\"\"\n",
    "    \n",
    "    fig = plt.figure(figsize=(20,10))\n",
    "    ax1 = fig.add_subplot(1,2,1)\n",
    "    ax1.set_xlim([-0.25,1.25])\n",
    "    ax1.set_ylim([-0.025,1.025])\n",
    "    ax1.set_xlabel('Threshold')\n",
    "    ax1.set_ylabel('Precision and Recall')\n",
    "    ax1.set_title(\"Precision, Recall Curve vs Threshold \" + tag)\n",
    "\n",
    "    pr,rc,th = precision_recall_curve(y_act, y_score)\n",
    "    \n",
    "    ax1.plot(th,pr[:-1], \"b--\",label=\"Precision\")\n",
    "    ax1.plot(th,rc[:-1], \"g-\",label=\"Recall\")\n",
    "    \n",
    "    ax1.legend(loc='upper left')\n",
    "    \n",
    "    if show:\n",
    "        plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following is the convenient utility for reporting, ROC, PR and F1 score ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def report_clf(y_act, y_pred, y_proba, title=\"\", cmap=\"Blues\"):\n",
    "    \"\"\"\n",
    "     create the classification reports with confusion matrix\n",
    "     arguement:\n",
    "     y_act -- Actual label of the class on the test data.\n",
    "     y_pred -- Prediction by model on the test data.\n",
    "     y_proba -- Probabilities as predicted by model on the test data.\n",
    "    \"\"\"\n",
    "    plot_pr_roc(y_act, y_proba, \"\", \"darkorange\", True, title)  \n",
    "    plot_cm(y_act, y_pred, title + \" Confusion Matrix\", cmap) \n",
    "    print(\"\\n\\n Classification Report \", title, \"\\n\\n\")  \n",
    "    print(classification_report(y_act, y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "###### Utility for prediction using weighted cross validation\n",
    "\n",
    "Note that we had splitted dataset into training and test. When we are training model, we shouldn't look at the held out test dataset until final testing. This way we will know how well or bad our model generalizes.\n",
    "\n",
    "However when we are training the model, we would also like to validate our model on the part of the training set. We further split the train set into validate train and validate test set.\n",
    "\n",
    "The best strategy is to use cross validation. In this approach , we split dataset into K folds $(f_1, f_2, ..f_k)$ and train the model on $(k-1)$ folds and test it on the remaining fold. The process is repeated by selecting each fold as the validate test set and remaining $(k-1)$ folds as validate train set. Model with the best result is choosen\n",
    "\n",
    "For more details see the article on wiki about [cross validation](https://en.wikipedia.org/wiki/Cross-validation_(statistics).\n",
    "\n",
    "Scikit learn provides a many nice utilities for cross validation. We will use the [cross_val_predict](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_predict.html).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_xgb_weights(w, X, y):\n",
    "    \"\"\"\n",
    "    helper routine to create  weight of size y .All the values are same i.e w\n",
    "    \"\"\"\n",
    "    r = y.copy()\n",
    "    r[r == 1] = w  \n",
    "    r[r == 0] = 1\n",
    "    return r \n",
    "\n",
    "\n",
    "def find_cv_model_predict(xgb_model, X, y, weight=1):\n",
    "    \"\"\"\n",
    "    utility function to find the best model using k fold cross validation\n",
    "    \n",
    "    arguements: \n",
    "    xgb_model-- xgboost model\n",
    "    X -- transformed input dataset\n",
    "    y -- corresponding output dataset\n",
    "    weight -- weight to apply to positive sample. Default to 1 and we will see how it can be used with different weights.\n",
    "    \n",
    "    return: cross validated model with k == 3. It will return prob as well as prediction\n",
    "    \"\"\"\n",
    "    \n",
    "    # get the weights of w of size y\n",
    "    y_wts = create_xgb_weights(weight, X, y)\n",
    "    \n",
    "    # get prediction\n",
    "    y_proba = cross_val_predict(\n",
    "                xgb_model, X, y, cv=3, method=\"predict_proba\",\n",
    "                fit_params={'eval_metric':'auc',\n",
    "                           'sample_weight':y_wts})\n",
    "    \n",
    "    # get probabilies\n",
    "    y_pred = cross_val_predict(\n",
    "                xgb_model, X, y, cv=3, method=\"predict\",\n",
    "                fit_params={'eval_metric':'auc',\n",
    "                           'sample_weight':y_wts})\n",
    "    \n",
    "    # result\n",
    "    res = {\n",
    "        'model': xgb_model,\n",
    "        'X':X,\n",
    "        'y':y,\n",
    "        'weight': weight,\n",
    "        'proba':y_proba,\n",
    "        'pred':y_pred,\n",
    "    }\n",
    "    \n",
    "    return res\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### First Attempt at Model Training <a name=\"ml_train_1\"></a>\n",
    "\n",
    "In previous sections, we talked about xgboost, various utilities and various performance measures for classification model. Now it's time to train the model using cross validation and get the predicted probabilities and predicted results.\n",
    "\n",
    "We will use the helper function find_cv_model_predict which uses cross validation to find the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the model\n",
    "\n",
    "# we would like to split it using the stratified samples so that all the representative of each class is there\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.3, random_state=0)\n",
    "\n",
    "xgb_model = create_xgb_clf({'scale_pos_weight':1})\n",
    "\n",
    "\n",
    "# use cross validation to find best model\n",
    "xgb_predict_obj = find_cv_model_predict(xgb_model, X_train, y_train, weight=1)\n",
    "\n",
    "xgb_cls_1_proba = xgb_predict_obj['proba'][:,1]\n",
    "xgb_y_pred = xgb_predict_obj['pred']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of the classification \n",
    "\n",
    "Lets measure the performance of our model by using our helper functions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "report_clf(y_train, xgb_y_pred, xgb_cls_1_proba, title=\"for 1rst Attempt to Train Model\", cmap=\"Greens\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the precision and recall against threshold\n",
    "\n",
    "\n",
    "plot_pr_vs_th(y_train, xgb_cls_1_proba, show=True, tag=\"for 1rst Attempt to Train Model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "##### ROC Curve\n",
    "\n",
    "1) We can see that ROC curve is not hugging the top left corner. So ROC curve is not giving use the proper lift. What's wrong with it? The main reason for this is the class imbalances. i.e number of positive testcase i.e user said yes and subscribed a term deposit is much less than people who said \"no\"\n",
    "\n",
    "2) Typically in these cases of the imbalance class , you use the precision recall curve with a specific threshold, rather than roc curve, We will explore with the precision recall curve and the confusion matrix to solve the problem. \n",
    "\n",
    "##### Precision Recall Curve\n",
    "\n",
    "1) As we pointed out that for the imbalance dataset, the precision recall curve is better.\n",
    "\n",
    "2) if we want to find all the guys who are likely to accept the offer, we should aim for higher recall as explained in previous sections.\n",
    "\n",
    "\n",
    "##### Confusion Matrix\n",
    "\n",
    "1) For the imbalance data, we can see the confusion matrix is not balanced, i.e TPR and FPR are less.\n",
    "\n",
    "2) We note that overall recall is 0.9 but for positive class (which we care more), it is only 0.38 and we would want to improve it.\n",
    "\n",
    "##### precision and recall vs threshold\n",
    "\n",
    "1) We also plotted, precision recall vs threshold and since for us recall is more important so we should choose our threshold accordingly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strategy For Better Classifier for the Imbalance Data <a name=\"ml_clf_strategy\"></a>\n",
    "\n",
    "Can we do better than the current performance? The answer is yes and typically in these cases, we follow the following \n",
    "techniques.\n",
    "\n",
    "1) Use the weighted class i.e give higher weight for the class 1 i.e 'yes'\n",
    "\n",
    "2) OverSampling of the Minority class and Undersampling of the minority class\n",
    "\n",
    "3) Use the SMOTE (synthetic Minority class oversampling)\n",
    "   \n",
    "   \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Second Attempt At Model Training using Weighted Samples<a name=\"ml_train_2\"></a>\n",
    "\n",
    "Lets use first approach i.e weighting the class to find the better model. The reason weighing the samples works is that we are saying to xgboost for a weight say 10, that consider each positive sample 10 times more important than 1 negative samples. In short, this is same as taking one positive sample and replicating it 10 times.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# setup the figure\n",
    "fig1 = plt.figure(figsize=(30,15))\n",
    "\n",
    "# plot one for PR curve\n",
    "ax1 = fig1.add_subplot(1,2,1)\n",
    "ax1.set_xlim([-0.025,1.025])\n",
    "ax1.set_ylim([-0.025,1.025])\n",
    "ax1.set_xlabel('Recall')\n",
    "ax1.set_ylabel('Precision')\n",
    "ax1.set_title('PR Curve for Weigted Samples (2nd Attempt)')\n",
    "\n",
    "# plot 2 for ROC curve\n",
    "ax2 = fig1.add_subplot(1,2,2)\n",
    "ax2.set_xlim([-0.025,1.025])\n",
    "ax2.set_ylim([-0.025,1.025])\n",
    "ax2.set_xlabel('False Positive Rate (FPR)')\n",
    "ax2.set_ylabel('True Positive Rate (TPR)')\n",
    "ax2.set_title('ROC Curve for Weighted Samples (2nd Attempt)')\n",
    "\n",
    "# plot the baseline ROC curve     \n",
    "ax2.plot([0,1], [0,1], 'k--')\n",
    "    \n",
    "\n",
    "fig2 = plt.figure(figsize=(30,15))    \n",
    "# plot 3 for precision recall vs threshold curve\n",
    "ax3 = fig2.add_subplot(1,1,1)\n",
    "ax3.set_xlim([-0.025,1.025])\n",
    "ax3.set_ylim([-0.025,1.025])\n",
    "ax3.set_xlabel('Threshold')\n",
    "ax3.set_ylabel('Precision and Recall')\n",
    "ax3.set_title('Precision, Recall Curve vs Threshold for Weighted Samples(2nd Attempt)')\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "xgb_models = {}\n",
    "# try various weights and plot on the same figure for better comparasion\n",
    "for pos_sample_weight, color in zip([1,5,20,50,100, 1000, 10000],'bgrcmykw'):\n",
    "  \n",
    "\n",
    "  # create a model with pos_sample_weight and other default params\n",
    "  xgb_model = create_xgb_clf({'scale_pos_weight':pos_sample_weight})\n",
    "  \n",
    "  # store the model for future use\n",
    "  xgb_models[pos_sample_weight] = xgb_model\n",
    "    \n",
    "  # find the prediction with probabilities for pos_sample_weight\n",
    "  xgb_predict_obj = find_cv_model_predict(xgb_model, X_train, y_train, weight=pos_sample_weight)\n",
    "\n",
    "  xgb_cls_1_proba = xgb_predict_obj['proba'][:,1]\n",
    "  xgb_y_pred = xgb_predict_obj['pred']\n",
    "  y_act = y_train\n",
    "  y_score = xgb_cls_1_proba\n",
    "  label =  pos_sample_weight \n",
    "  pr,rc, pr_th = precision_recall_curve(y_act, y_score)\n",
    "  tpr,fpr,_ = roc_curve(y_act, y_score)\n",
    "    \n",
    "  ax1.plot(rc,pr,c=color,label=label)\n",
    "  ax2.plot(tpr,fpr,c=color,label=label)\n",
    " \n",
    "  ax3.plot(pr_th,pr[:-1], c=color,label=label)\n",
    "  ax3.plot(pr_th,rc[:-1], \"--\", c=color,label=label)\n",
    "    \n",
    "    \n",
    "ax1.legend(loc='lower left')    \n",
    "ax2.legend(loc='lower left')\n",
    "ax3.legend(loc='lower left')\n",
    "\n",
    "plt.show()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Analysis of Weighted model\n",
    "\n",
    "We have seen how ROC curve is not helpful for class imbalance dataset. We usually rellies on Precision vs Recall curve and Precisio/Recall vs threshold.\n",
    "\n",
    "We talked initially, how the recall is more important for our use case i.e we want to find all the people who are likely to accept the offer, even at some risk some of them rejecting it. \n",
    "\n",
    "Hence, we should have higher recall but good enough precision.\n",
    "\n",
    "Lets look at bottom picture, (double click to enlarge) for Precision,Recall vs Threshold curve.\n",
    "dashed lines are recall and solid lines are precision. We see that golden color lines have good balance of higher recall and good enough precision. Which corresponds to weight 1000.\n",
    "\n",
    "For the weight of 1000 i,e golden line, both the top figure for ROC and Precision vs Recall curve is also reasonable and hence we will go with that weight.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets also see the confusion matrix and classification report for the best weighted models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the best model with respect to recall\n",
    "best_weight = 1000\n",
    "xgb_model_best_wt_sofar = xgb_models[best_weight]\n",
    "\n",
    "\n",
    "# use cross validation to find best model\n",
    "xgb_predict_best_weighted_obj = find_cv_model_predict(xgb_model_best_wt_sofar, X_train, y_train, weight=best_weight)\n",
    "\n",
    "xgb_best_wt_cls_1_proba = xgb_predict_best_weighted_obj['proba'][:,1]\n",
    "xgb_best_wt_y_pred = xgb_predict_best_weighted_obj['pred']\n",
    "\n",
    "\n",
    "print(xgb_best_wt_cls_1_proba)\n",
    "print(xgb_best_wt_y_pred[10:100])\n",
    "\n",
    "report_clf(y_train, xgb_best_wt_y_pred, xgb_best_wt_cls_1_proba, title=\"for 2nd Attempt to Train Weighted Model \", cmap=\"Greens\")\n",
    "plot_pr_vs_th(y_train, xgb_best_wt_cls_1_proba, show=True, tag=\"for 2nd Attempt to Train Weighted Model \")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**The key observation**, we make is that our best weighted model, the recall for positive class is 0.89 improvement from 0.38 but overall recall is reduced and it's ok as for yes recall for positive samples are more important\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Third Attempt at  Model Training using Weighted Samples and Feature Selection<a name=\"ml_train_3\"></a> \n",
    "\n",
    "\n",
    "Now as we saw that weight 1000 is the best and we will use it. The next question, we ask is how can we make it better.\n",
    "One of the things about the decision tree based model is that more the number of attributes, more noise is introduced i.e model won't generalize. So it is advisable to use only the important features for building.\n",
    "\n",
    "We will classify using the important features to remove the noise.\n",
    "\n",
    "1) We will find the important features\n",
    "\n",
    "2) We will use it to classify again\n",
    "\n",
    "3) plot the PR, ROC curve.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Plot feature importance\n",
    "\n",
    "Before we proceed to optimize the code, lets plot feature importance for our best model so far.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "#xgb_model.fit(X_train, y_train, eval_metric='auc')\n",
    "\n",
    "# get the best model with respect to recall\n",
    "best_weight = 1000\n",
    "xgb_model_best_sofar = xgb_models[best_weight]\n",
    "\n",
    "# fit the whole training set. We will later run for the better model later, but for now. Lets go with it.\n",
    "xgb_model_best_sofar.fit(X_train, y_train, eval_metric='auc')\n",
    "\n",
    "\n",
    "# fit the model just in case\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "\n",
    "ax1 = fig.add_subplot(1,1,1)\n",
    "\n",
    "plot_importance(xgb_model_best_sofar, ax=ax1)\n",
    "\n",
    "\n",
    "# need graph viz package\n",
    "#ax2 = fig.add_subplot(1,2,2)\n",
    "#plot_tree(xgb_model, ax=ax2)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(\"Feature importance = \", xgb_model_best_sofar.feature_importances_)\n",
    "print(\"Sorted Feature importance = \", np.sort(xgb_model_best_sofar.feature_importances_))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that not all features are important and may be our model is better, if we use only the selected features\n",
    "Lets try it out.\n",
    "\n",
    "\n",
    "We have transformer [SelectFromModel](http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectFromModel.html) which will select the features based on threshold.\n",
    "\n",
    "\n",
    "Now, we won't know what threshold, we should use for selecting the list of features. So we will perform the grid search using our custom code. \n",
    "\n",
    "We will plot various performance metrics and use visual inspection to find the best threshold.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# setup the figure\n",
    "fig1 = plt.figure(figsize=(30,15))\n",
    "\n",
    "# plot one for PR curve\n",
    "ax1 = fig1.add_subplot(1,2,1)\n",
    "ax1.set_xlim([-0.025,1.025])\n",
    "ax1.set_ylim([-0.025,1.025])\n",
    "ax1.set_xlabel('Recall')\n",
    "ax1.set_ylabel('Precision')\n",
    "ax1.set_title('PR Curve for various Feature Importance Threshold')\n",
    "\n",
    "# plot 2 for ROC curve\n",
    "ax2 = fig1.add_subplot(1,2,2)\n",
    "ax2.set_xlim([-0.025,1.025])\n",
    "ax2.set_ylim([-0.025,1.025])\n",
    "ax2.set_xlabel('False Positive Rate (FPR)')\n",
    "ax2.set_ylabel('True Positive Rate (TPR)')\n",
    "ax2.set_title('ROC Curve for various Feature Importance Threshold')\n",
    "\n",
    "# plot the baseline ROC curve     \n",
    "ax2.plot([0,1], [0,1], 'k--')\n",
    "    \n",
    "\n",
    "fig2 = plt.figure(figsize=(30,15))    \n",
    "# plot 3 for precision recall vs threshold curve\n",
    "ax3 = fig2.add_subplot(1,1,1)\n",
    "ax3.set_xlim([-0.025,1.025])\n",
    "ax3.set_ylim([-0.025,1.025])\n",
    "ax3.set_xlabel('Threshold')\n",
    "ax3.set_ylabel('Precision and Recall')\n",
    "ax3.set_title('Precision, Recall Curve vs Threshold for various Feature Importance Threshold')\n",
    "\n",
    "\n",
    "\n",
    "#lets explore various thresholds.\n",
    "fi_thresholds = xgb_model_best_sofar.feature_importances_\n",
    "\n",
    "fi_ths_sorted = np.sort(fi_thresholds)\n",
    "ths_size = len(fi_ths_sorted)\n",
    "ths_lim = 10 # threshold limits i.e the various values we want to try out.\n",
    "if ths_size > ths_lim:\n",
    "    # select only the thresholds\n",
    "    step = int(len(fi_ths_sorted)/ths_lim)\n",
    "    fi_ths_sorted = [fi_ths_sorted[i] for i in range(0, len(fi_ths_sorted), step)]\n",
    "      \n",
    "ths_size = len(fi_ths_sorted)   \n",
    "colors = plt.cm.rainbow(np.linspace(0,1, ths_size))\n",
    "        \n",
    "# from best_weights\n",
    "xgb_models_bm_fi = {}\n",
    "select_txfs = {}\n",
    "# try various weights and plot on the same figure for better comparasion\n",
    "for fi_th, color in zip(fi_ths_sorted, colors):\n",
    "  \n",
    "\n",
    "  # train a transformer to select a feature based on threshold 50\n",
    "  select_txf = SelectFromModel(xgb_model_best_sofar, threshold=fi_th, prefit=True)\n",
    "\n",
    "  select_X_train = select_txf.transform(X_train)\n",
    "  select_txfs[fi_th] = select_txf\n",
    "\n",
    "\n",
    "  # create a model with pos_sample_weight and other default params\n",
    "  xgb_model = create_xgb_clf({'scale_pos_weight':best_weight})\n",
    "  \n",
    "  # store the model for future use\n",
    "  xgb_models_bm_fi[fi_th] = xgb_model\n",
    "    \n",
    "  # find the prediction with probabilities for pos_sample_weight\n",
    "  xgb_predict_obj = find_cv_model_predict(xgb_model, select_X_train, y_train, weight=best_weight)\n",
    "\n",
    "  xgb_cls_1_proba = xgb_predict_obj['proba'][:,1]\n",
    "  xgb_y_pred = xgb_predict_obj['pred']\n",
    "  y_act = y_train\n",
    "  y_score = xgb_cls_1_proba\n",
    "  label =  fi_th\n",
    "  pr,rc, pr_th = precision_recall_curve(y_act, y_score)\n",
    "  tpr,fpr,_ = roc_curve(y_act, y_score)\n",
    "    \n",
    "  ax1.plot(rc,pr,c=color,label=label)\n",
    "  ax2.plot(tpr,fpr,c=color,label=label)\n",
    " \n",
    "  ax3.plot(pr_th,pr[:-1], c=color,label=label)\n",
    "  ax3.plot(pr_th,rc[:-1], \"--\", c=color,label=label)\n",
    "    \n",
    "    \n",
    "ax1.legend(loc='lower left')    \n",
    "ax2.legend(loc='lower left')\n",
    "ax3.legend(loc='lower left')\n",
    "\n",
    "plt.show()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we don't want to chose red one as it is too much overfitting.\n",
    "\n",
    "If we chose bluish green color corresponding to threshold 0.008, we get reasonable performance for recall and precision. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Analysis of the Weighted Model with Feature Selection.\n",
    "\n",
    "Lets use 0.008 threshold to plot again various metrics especially classificaion report and see how are we going compare to our [weighted model](#ml_train_2)\n",
    "\n",
    "We will plot various performance metrics first as we have done in previous sections\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train a transformer to select a feature based on threshold 50\n",
    "select_txf = SelectFromModel(xgb_model_best_sofar, threshold=0.008, prefit=True)\n",
    "\n",
    "select_X_train = select_txf.transform(X_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "xgb_model_weighted = create_xgb_clf({'scale_pos_weight':best_weight})\n",
    "\n",
    "\n",
    "#xgb_predict_obj = find_cv_model_predict(xgb_model, X_train, y_train, weight=1)\n",
    "# use cross validation to find best model\n",
    "xgb_predict_weighted_obj = find_cv_model_predict(xgb_model_weighted, select_X_train, y_train, weight=best_weight)\n",
    "\n",
    "xgb_wt_cls_1_proba = xgb_predict_weighted_obj['proba'][:,1]\n",
    "xgb_wt_y_pred = xgb_predict_weighted_obj['pred']\n",
    "\n",
    "\n",
    "print(xgb_wt_cls_1_proba)\n",
    "print(xgb_wt_y_pred[10:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_clf(y_train, xgb_wt_y_pred, xgb_wt_cls_1_proba, title=\"for best threshold\", cmap=\"Greens\")\n",
    "plot_pr_vs_th(y_train, xgb_wt_cls_1_proba, show=True, tag=\" for best threshold\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "1) Compare to weighted model using a subset of features does improve a small bit of recall for positive class from 0.89 to 0.9 and also overall recall from 0.7 to 0.71.\n",
    "\n",
    "2) Feature Selection is important techniques for improving classification performance. Since decision based models are robust to features, for this case, we don't see much improvement in performance for feature selection.If we had differnt models say logistic regression, we would see a much bigger improvement.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Generalization and Prediction <a name=\"ml_predict\"></a>\n",
    "\n",
    "Now we have trained our model and we are ready to do inference. Note that in a real application. Once the model is trained , we will export it and deploy on the production machine. But for the purpose of this notebook, we will run it on the hold out data. Note that we didn't even use the held out data for cross validation since doing that would bias our model. So running it on the hold out dataset, will truly tell us generalization of the model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### prepare test data using ML pipeline\n",
    "\n",
    "Lets now run the same pipeline we created before to prepare our test data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "X_gen_test = X_pipeline.transform(data_raw_test)\n",
    "print(\"\\n\\nShape of the transformed data. Note that increased in the number of columns due to one hot encoding\\n\")\n",
    "print(\"\\nOriginal data shape (num_rows, num_cols) == \")\n",
    "print(data_raw_test.shape)\n",
    "print(\"\\nTransformed data shape (num_rows, num_cols) == \")\n",
    "print(X_gen_test.shape)\n",
    "print(\"\\n\\n Sample of transformed inputs\\n\", X, \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_gen_test = y_pipeline.transform(data_raw_test)\n",
    "\n",
    "\n",
    "print(\"\\nSample of original output = \", data_raw_test['y'].as_matrix().reshape(-1)[:20], \"\\n\")\n",
    "print(\"\\nSample of transformed output = \", y_gen_test[:20], \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the best model for prediction\n",
    "\n",
    "1) Train on whole training set with best weight and best threshold.\n",
    "\n",
    "2) test on held out test set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_threshold = 0.008\n",
    "select_txf3 = SelectFromModel(xgb_model_best_sofar, threshold=best_threshold, prefit=True)\n",
    "\n",
    "select_X_train = select_txf3.transform(X_train)\n",
    "\n",
    "xgb_model_gen = create_xgb_clf({'scale_pos_weight': best_weight})\n",
    "\n",
    "\n",
    " # get the weights of w of size y\n",
    "y_wts = create_xgb_weights(best_weight, X, y)\n",
    "\n",
    "xgb_model_gen.fit(select_X_train, y_train, sample_weight=y_wts)\n",
    "\n",
    "select_X_test = select_txf3.transform(X_gen_test)\n",
    "\n",
    "y_test_proba = xgb_model_gen.predict_proba(select_X_test)\n",
    "y_test_pred = xgb_model_gen.predict(select_X_test)\n",
    "\n",
    "\n",
    "\n",
    "y_test_cls_1_proba = y_test_proba[:,1]\n",
    "y_test_cls_1_pred = y_test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_clf(y_gen_test, y_test_cls_1_pred, y_test_cls_1_proba, title=\"for held out Test Data \", cmap=\"Greens\")\n",
    "plot_pr_vs_th(y_gen_test, y_test_cls_1_proba, show=True, tag=\"  for held out Test Data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Analysis of Model Prediction on held out test data\n",
    "\n",
    "For We note that overall, recall is better (0.82) and positive class recall is 0.63.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary <a name=\"summary\"></a>\n",
    "\n",
    "Thus, we looked at various ways of improving the model performance.\n",
    "\n",
    "First of all, for the class imbalance dataset, accuracy and ROC curve is not useful. So we use Precision vs Recall and Precision Recall vs threshold. Depending on the business use cases, we have to improve recall or precision. In our case, we improved recall metric without sacrificing too much of precision.\n",
    "\n",
    "We did three iterations for improving model performance using three different techniques\n",
    "\n",
    "1) Use the good parameters to build xgboost model \n",
    "\n",
    "2) Use the weighted samples to build a better model\n",
    "\n",
    "3) Since we were using almost all the 50 features, we were probably overfitting, so we plotting feature importance \n",
    " and found the best set of the features to improve the overall recall score\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other methods for improving for class imbalance. <a name=\"other_techniques\"></a>\n",
    "\n",
    "The performance of classification model for class imbalances can be improve further.\n",
    "However, In this notebook, we will not explore the oversampling method and SMOTE method but we will give a short\n",
    "description on what they are\n",
    "\n",
    "1) **Oversampling of minority class and Undersampling of majority class.**\n",
    "\n",
    "As the name implies, this is just the straightforward procedure and following scikit function can be used.See [References](#references) for examples.\n",
    "\n",
    "\n",
    "2) **SMOTE method.**\n",
    "\n",
    "Problem with weighted and Over and Under Sampling methods are that the same data is reused and the better approach \n",
    "is to generate a new sample based on the two samples. One in the positive class and one in the negative class and use\n",
    "interpolation and k nearest neighbour to generate new samples between the decision boundry, that is basically the idea of SMOTE method. Scikit learn has method for this. See [References](#references) for SMOTE examples and SMOTE paper.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References <a name=\"references\"></a>\n",
    "\n",
    "\n",
    "* [Scikit Learn](http://scikit-learn.org/stable/)\n",
    "\n",
    "* [Pandas](https://pandas.pydata.org/)\n",
    "\n",
    "* [Matplotlib](https://matplotlib.org/)\n",
    "\n",
    "* [SeaBorn](https://seaborn.pydata.org/)\n",
    "\n",
    "* [XGBoost](https://github.com/dmlc/xgboost)\n",
    "\n",
    "* [SMOTE Paper](https://www.jair.org/media/953/live-953-2037-jair.pdf)\n",
    "\n",
    "* [SMOTE Example](http://contrib.scikit-learn.org/imbalanced-learn/stable/auto_examples/over-sampling/plot_smote.html)\n",
    "\n",
    "* [Over Sampling Example](http://contrib.scikit-learn.org/imbalanced-learn/stable/over_sampling.html)\n",
    "\n",
    "* [Under Sampling Example](http://contrib.scikit-learn.org/imbalanced-learn/stable/auto_examples/under-sampling/plot_random_under_sampler.html)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_m1",
   "language": "python",
   "name": "tf_m1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
